{"cells": [{"cell_type": "code", "execution_count": 1, "id": "35ca0afd-6895-47be-b77d-42942ac0cbc7", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/29 20:04:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}, {"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-3930-m.us-central1-f.c.hadoop-practice-464221.internal:44655\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f8ebea12650>"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"PartitioningDemo\").master(\"yarn\").getOrCreate()\n\nspark\n"}, {"cell_type": "code", "execution_count": 2, "id": "3768ad4d-4dbd-45e5-9d02-d4c42bca71f7", "metadata": {"tags": []}, "outputs": [], "source": "data = [\n\"Goku Vegeta Gohan\",\n\"Goku Frieza Goku\",\n\"Vegeta Goku Gohan Frieza\",\n\"Gohan Frieza Goku Goku\"\n]"}, {"cell_type": "code", "execution_count": 5, "id": "40502952-6a55-4a05-89ed-ca98b33ae669", "metadata": {"tags": []}, "outputs": [], "source": "rdd = spark.sparkContext.parallelize(data, 2)"}, {"cell_type": "code", "execution_count": 6, "id": "e1a2a110-fe7e-4671-a3b1-01162c6960e2", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "2"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "rdd.getNumPartitions()"}, {"cell_type": "code", "execution_count": 7, "id": "38fa3f5d-72ec-448b-898a-509d60d98685", "metadata": {"tags": []}, "outputs": [], "source": "hdfs_path = \"/tmp/names.txt\"\n\nrdd1 = spark.sparkContext.textFile(hdfs_path)"}, {"cell_type": "code", "execution_count": 8, "id": "b697ae65-85d5-4e47-86cb-bdea65e34255", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "2"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "rdd1.getNumPartitions()"}, {"cell_type": "markdown", "id": "0e510004-af20-44e7-8eb3-d086b51fdf1f", "metadata": {}, "source": "- even thoguh the file size is less than block size we get 2 partitions because of setting"}, {"cell_type": "code", "execution_count": 9, "id": "d5392001-9369-40c0-b5b6-dad8e673e9d6", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Default parallelism: 2\n"}], "source": "# Default Parallelism\nprint(f\"Default parallelism: {spark.sparkContext.defaultParallelism}\")"}, {"cell_type": "code", "execution_count": 10, "id": "ac5db94f-2a39-4f4d-8a76-b53ca8b18858", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "6"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "rdd1 = rdd1.repartition(6)\n\nrdd1.getNumPartitions()"}, {"cell_type": "code", "execution_count": 37, "id": "a09f2c04-a555-489b-9205-374a258cb10a", "metadata": {"tags": []}, "outputs": [], "source": "spark.stop()\n\n# when we stop Spark, it stops the job. they will be seen in history server, only when the session is stopped"}, {"cell_type": "markdown", "id": "84991a55-2304-45ff-be57-5a8591ff3b48", "metadata": {}, "source": "# Restart Kernel"}, {"cell_type": "code", "execution_count": 1, "id": "20666f56-77f1-43b6-8089-dbd298315bc8", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/29 21:51:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}, {"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-3930-m.us-central1-f.c.hadoop-practice-464221.internal:43207\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f9e3d83e750>"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"RDD_Operations\").master(\"yarn\").getOrCreate()\n\nspark\n"}, {"cell_type": "code", "execution_count": 2, "id": "7469d270-5fb0-47c3-965f-e27adbab8a04", "metadata": {"tags": []}, "outputs": [], "source": "customer_data = [\n    \"customer_id,name,city,state,country,registration_date,is_active\",\n    \"0,Customer_0,Pune,Maharashtra,India,2023-06-29,False\",\n    \"1,Customer_1,Bangalore,Tamil Nadu,India,2023-12-07,True\",\n    \"2,Customer_2,Hyderabad,Gujarat,India,2023-10-27,True\",\n    \"3,Customer_3,Bangalore,Karnataka,India,2023-10-17,False\",\n    \"4,Customer_4,Ahmedabad,Karnataka,India,2023-03-14,False\",\n    \"5,Customer_5,Hyderabad,Karnataka,India,2023-07-28,False\"\n]"}, {"cell_type": "code", "execution_count": 3, "id": "d7cf695c-ad1d-4a30-919b-4ffd09f87e80", "metadata": {"tags": []}, "outputs": [], "source": "data_rdd = spark.sparkContext.parallelize(customer_data)"}, {"cell_type": "code", "execution_count": 4, "id": "bea91d07-5366-450b-84d4-0b8cc50bf5b1", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "['customer_id,name,city,state,country,registration_date,is_active',\n '0,Customer_0,Pune,Maharashtra,India,2023-06-29,False',\n '1,Customer_1,Bangalore,Tamil Nadu,India,2023-12-07,True',\n '2,Customer_2,Hyderabad,Gujarat,India,2023-10-27,True',\n '3,Customer_3,Bangalore,Karnataka,India,2023-10-17,False',\n '4,Customer_4,Ahmedabad,Karnataka,India,2023-03-14,False',\n '5,Customer_5,Hyderabad,Karnataka,India,2023-07-28,False']"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "data_rdd.collect()"}, {"cell_type": "code", "execution_count": 5, "id": "b310285c-2469-4f71-9b7d-e2e7b1c4273f", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "customer_id,name,city,state,country,registration_date,is_active\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# first() --> Action\n\nheader = data_rdd.first()\nprint(header)"}, {"cell_type": "code", "execution_count": 6, "id": "8ce8067d-278f-4b0e-b240-67060ad87c46", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "['0,Customer_0,Pune,Maharashtra,India,2023-06-29,False', '1,Customer_1,Bangalore,Tamil Nadu,India,2023-12-07,True', '2,Customer_2,Hyderabad,Gujarat,India,2023-10-27,True', '3,Customer_3,Bangalore,Karnataka,India,2023-10-17,False', '4,Customer_4,Ahmedabad,Karnataka,India,2023-03-14,False', '5,Customer_5,Hyderabad,Karnataka,India,2023-07-28,False']\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# filter () --> Transformation\n\ndata_rdd = data_rdd.filter(lambda row : row!=header)\nprint(data_rdd.collect())"}, {"cell_type": "markdown", "id": "59c01840-21e0-4dbc-ada8-b18e90444854", "metadata": {}, "source": "# Map funtions"}, {"cell_type": "code", "execution_count": 7, "id": "0cb33bfd-5ff6-4b07-8b39-73476863419b", "metadata": {"tags": []}, "outputs": [], "source": "def parse_data(row):\n    fields = row.split(',')\n    return (\n        int(fields[0]),\n        fields[1],\n        fields[2],\n        fields[3],\n        fields[4],\n        fields[5],\n        fields[6]=='True'\n    )\n"}, {"cell_type": "code", "execution_count": 8, "id": "95ec2432-51e8-4000-be67-198caa3e0e5c", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "['1', 'Customer_1', 'Bangalore', 'Tamil Nadu', 'India', '2023-12-07', 'True']\n"}], "source": "test_data = \"1,Customer_1,Bangalore,Tamil Nadu,India,2023-12-07,True\"\nprint(test_data.split(','))"}, {"cell_type": "code", "execution_count": 9, "id": "44254d7f-f8f1-4802-9f89-dd628b326158", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "[(0, 'Customer_0', 'Pune', 'Maharashtra', 'India', '2023-06-29', False),\n (1, 'Customer_1', 'Bangalore', 'Tamil Nadu', 'India', '2023-12-07', True),\n (2, 'Customer_2', 'Hyderabad', 'Gujarat', 'India', '2023-10-27', True),\n (3, 'Customer_3', 'Bangalore', 'Karnataka', 'India', '2023-10-17', False),\n (4, 'Customer_4', 'Ahmedabad', 'Karnataka', 'India', '2023-03-14', False),\n (5, 'Customer_5', 'Hyderabad', 'Karnataka', 'India', '2023-07-28', False)]"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "parsed_rdd = data_rdd.map(parse_data)\nparsed_rdd.collect()"}, {"cell_type": "code", "execution_count": 10, "id": "aeefc1fe-21af-406e-b053-aac9ecfb49e4", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "[('Customer_0', 'Pune'),\n ('Customer_1', 'Bangalore'),\n ('Customer_2', 'Hyderabad'),\n ('Customer_3', 'Bangalore'),\n ('Customer_4', 'Ahmedabad'),\n ('Customer_5', 'Hyderabad')]"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "name_city_rdd = parsed_rdd.map(lambda row : (row[1],row[2]))\nname_city_rdd.collect()"}, {"cell_type": "code", "execution_count": 11, "id": "a5875551-b867-40b5-9286-7cfa30cdcd92", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "['Hyderabad']"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "cities_rdd = parsed_rdd.map(lambda row : row[2]).distinct()\ncities_rdd.take(1)\n\n# take gives number of records given"}, {"cell_type": "code", "execution_count": 12, "id": "c2467a79-94a8-425a-b240-d0a39ff96f47", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('Pune', 1), ('Hyderabad', 2), ('Ahmedabad', 1), ('Bangalore', 2)]\n"}], "source": "# Reduce By Key\n\n# count of customers per city\ncustomers_rdd = parsed_rdd.map(lambda row : (row[2],1)).reduceByKey(lambda x,y:x+y)\nprint(customers_rdd.collect())"}, {"cell_type": "code", "execution_count": 13, "id": "2e547e51-9f6d-4675-b453-5fbc220d1eda", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "['Hyderabad', 'Pune', 'Ahmedabad', 'Bangalore']"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "cities_rdd.collect()"}, {"cell_type": "code", "execution_count": 14, "id": "b12c8494-0e2d-4881-a18d-fcd944f41950", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "defaultdict(int, {'Pune': 1, 'Bangalore': 2, 'Hyderabad': 2, 'Ahmedabad': 1})"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "parsed_rdd.map(lambda row : row[2]).countByValue()\n\n# this is a action, that is why we dont do .collect()\n# reduceByKey is transformation and we can do other operations after it\n#  whereas actions are the last step and operations cannot be done after it"}, {"cell_type": "code", "execution_count": 15, "id": "372ef7cb-fc20-485d-9021-761d83fbe748", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "PythonRDD[21] at RDD at PythonRDD.scala:53"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "parsed_rdd.map(lambda row : (row[2],1)).reduceByKey(lambda x,y:x+y)"}, {"cell_type": "code", "execution_count": 16, "id": "896a398f-76c7-480c-859a-ecec8f6667e2", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "['Hyderabad', 'Bangalore']"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": "active_cities = parsed_rdd.filter(lambda row : row[6]==True).map(lambda row:row[2]).distinct()\nactive_cities.collect()"}, {"cell_type": "code", "execution_count": 18, "id": "f9a89a31-040b-422e-9741-faa307618f93", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[('Maharashtra', 1), ('Tamil Nadu', 1), ('Gujarat', 1), ('Karnataka', 3)]\n"}], "source": "customer_by_state = parsed_rdd.map(lambda row : (row[3],1)).reduceByKey(lambda x,y:x+y)\nprint(customer_by_state.collect())"}, {"cell_type": "code", "execution_count": 19, "id": "3d59a236-a9e6-4952-92a5-373047cccdb7", "metadata": {"tags": []}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "75672526-e1b3-45a5-b860-0ef237fc0a04", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}